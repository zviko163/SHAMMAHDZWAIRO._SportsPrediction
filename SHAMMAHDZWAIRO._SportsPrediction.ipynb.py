# -*- coding: utf-8 -*-
"""regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kVtuSF7Vi_x_9e09vU-7x_NTNJHezAhk
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

# load datasets
male_players = pd.read_csv('/content/drive/MyDrive/male_players (legacy).csv')

# testing dataset
player_22 = pd.read_csv('/content/drive/MyDrive/players_22-1.csv')

male_players.head()

player_22.head()

mp_categorical = male_players.select_dtypes(include=['object'])
mp_numerical = male_players.select_dtypes(include=['int64', 'float64'])
len(mp_categorical.columns), len(mp_numerical.columns)

player_cat = player_22.select_dtypes(include = ['object'])
player_num = player_22.select_dtypes(include = 'number')
len(player_cat.columns), len(player_num.columns)

# to match columns in the training and testing dataset
for i in mp_numerical.columns:
  if i not in player_num.columns:
    print(i)
    mp_numerical = mp_numerical.drop(i, axis = 1)

# removing columns in the second dataset that are not in the first
for i in player_num.columns:
  if i not in mp_numerical.columns:
    print(i)
    player_num = player_num.drop(i, axis = 1)

len(mp_numerical.columns), len(player_num.columns)

# # features like national id are identifiers, no contribution to fifa overall rating
# removals = []

# # removing any column name that has _id in it to minimize bias caused by large id differences
# # and jersey number too
# for i in mp_numerical.columns:
#   if '_id' in i:
#     removals.append(i)

# for i in mp_numerical.columns:
#   if 'jersey_number' in i:
#     removals.append(i)
# removals

# for i in removals:
#   mp_numerical = mp_numerical.drop(i, axis = 1)
#   player_num = player_num.drop(i, axis = 1)
# len(mp_numerical.columns), len(player_num.columns)

mp_numerical.head()

player_num.head()

# confirming that the columns from both datasets are in the same order
count = 0
for i in range(53):
  if player_num.columns[i] == mp_numerical.columns[i]:
    count += 1
count

# using IterativeImputer to impute for numericals
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imputer = IterativeImputer(max_iter = 10, random_state = 0)

# mp_numerical = pd.DataFrame(imputer.fit_transform(mp_numerical))

# player_num = pd.DataFrame(imputer.fit_transform(player_num))

bits = []
# imputing 5 columns at a time and appending the dataframe objects in bits
for i in range(0, len(mp_numerical.columns), 5):
  bits.append(pd.DataFrame(imputer.fit_transform(mp_numerical.iloc[:, i:i+5])))

# keeping column names
mp_columns = mp_numerical.columns

mp_numerical = pd.DataFrame()
for i in bits:
  mp_numerical = pd.concat([mp_numerical, i], axis = 1, ignore_index = True)

# keeping the column names
mp_numerical.columns = mp_columns
mp_numerical.head()

bits = []
# imputing the player_num dataframe 5 columns at a time
for i in range(0, len(player_num.columns), 5):
  bits.append(pd.DataFrame(imputer.fit_transform(player_num.iloc[:,i:i+5])))

player_num = pd.DataFrame()
for i in bits:
  player_num = pd.concat([player_num, i], axis = 1)

player_num.columns = mp_columns
player_num.head()

# picking features with a strong correlation with overall rating
corr_matrix = mp_numerical.corr()
target_corr = corr_matrix['overall']
selected_features = target_corr[abs(target_corr) >= 0.5].index.tolist()
selected_features.remove('overall')
selected_features

Y_train = mp_numerical['overall']
# using selected_features for X_train
X_train = mp_numerical[selected_features]
X_train.head()

# to scale X_train using StandardScaler
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_scaled, columns = X_train.columns)
X_train.head()

Y_test = player_num['overall']
# using selected features for X_test
X_test = player_num[selected_features]
X_test.head()

# scaling X_test
X_test_scaled = scaler.transform(X_test)
X_test = pd.DataFrame(X_test_scaled, columns = X_test.columns)



# importing Decision tree regression, linear regression, gradient boost, and xgboost
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor

# instatiating imported models
rf = RandomForestRegressor(n_estimators = 100, random_state = 0)
dt = DecisionTreeRegressor(random_state = 0)
lr = LinearRegression()
gb = GradientBoostingRegressor(random_state = 0)
xgb = XGBRegressor(random_state = 0)

# importing evaluation metrics for continuous data
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle as pkl

models = (rf, dt, lr, gb, xgb)

# importing gridsearch for cross-validarion
from sklearn.model_selection import GridSearchCV

for model in models:
 model.fit(X_train, Y_train)
 pkl.dump(model, open('/content/drive/MyDrive/' + model.__class__.__name__ + '.pkl', 'wb'))
 Y_pred = model.predict(X_test)
 print(f'''{model.__class__.__name__},
          Mean Squared Error: {mean_squared_error(Y_test, Y_pred)},
          Root Mean Squared Error: {np.sqrt(mean_squared_error(Y_test, Y_pred))},
          Mean Absolute Error: {mean_absolute_error(Y_test, Y_pred)},
          R2 Score: {r2_score(Y_test, Y_pred)}''')

# picking 3 with the best r2_scores
models = (rf, dt, xgb)
# xgb_parameters for GridSearch
xgb_param_grid = {
    'n_estimators': [50, 200],
    'learning_rate': [0.1, 0.01],
    'max_depth': [3, 4],
    'min_child_weight': [1, 3]
}

# RandomForest parameters grid
rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 4],
    'min_samples_split': [2, 3],
    'min_samples_leaf': [1, 2]
}

# linear regression parameter grid
lr_param_grid = {
    'fit_intercept': [True, False]
}

# decision tree parameter grid
dt_param_grid = {
    'max_depth': [3, 4],
    'min_samples_split': [2, 3],
    'min_samples_leaf': [1, 2]
}

# gradient boost parameter grid
gb_param_grid = {
    'n_estimators': [50, 200],
    'learning_rate': [0.1, 0.01],
    'max_depth': [3, 4],
    'min_samples_split': [2, 4]
}



# importing pipeline
from sklearn.pipeline import Pipeline

# scoring = ['accuracy', 'r2', 'neg_mean_squared_error']

for model in models:
  if model.__class__.__name__ == 'XGBRegressor':
    param_grid = xgb_param_grid
  elif model.__class__.__name__ == 'RandomForestRegressor':
    param_grid = rf_param_grid
  elif model.__class__.__name__ == 'LinearRegression':
    param_grid = lr_param_grid
  elif model.__class__.__name__ == 'DecisionTreeRegressor':
    param_grid = dt_param_grid
  elif model.__class__.__name__ == 'GradientBoostingRegressor':
    param_grid = gb_param_grid


  grid_search = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'r2', cv = 3, n_jobs = -1)

  grid_search.fit(X_train, Y_train)

  pkl.dump(grid_search, open('/content/drive/MyDrive/' + model.__class__.__name__ + '_grid.pkl', 'wb'))
  print(f'''{model.__class__.__name__},
          Best Parameters: {grid_search.best_params_},
          Best Score: {grid_search.best_score_}''')

pd.DataFrame({'features': X_test.columns, 'importances': xgb.feature_importances_}).sort_values(by = 'importances', ascending = False)

# saving the scaler
pkl.dump(scaler, open('/content/drive/MyDrive/scaler.pkl', 'wb'))

best_dt_regressor = grid_search.best_estimator_

# Fit the best model to the training data
best_dt_regressor.fit(X_train, Y_train)

# saving the best_dt_regressor
pkl.dump(best_dt_regressor, open('/content/drive/MyDrive/best_dt_regressor.pkl', 'wb'))

